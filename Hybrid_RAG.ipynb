{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hybrid_RAG/Hybrid_RAG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbiYqGU8u7n8",
        "outputId": "450d0955-e796-43d0-edef-4ef02d8aa9b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Hybrid_RAG/Hybrid_RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "1rV4Jy2oyaqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2aca4a8-0afa-4f73-babc-27ed6b2b585d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.46.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.3.26)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.3.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.3.26)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.3.66)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (5.6.1)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.9)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.11.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.1.1)\n",
            "Collecting transformers==4.50.1 (from -r requirements.txt (line 12))\n",
            "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1->-r requirements.txt (line 12)) (4.67.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 2)) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 2)) (2.0.41)\n",
            "Requirement already satisfied: groq<1,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain-groq->-r requirements.txt (line 3)) (0.29.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 4)) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 4)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 4)) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->-r requirements.txt (line 5)) (1.33)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r requirements.txt (line 10)) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r requirements.txt (line 10)) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 4)) (1.20.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (1.44.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.1->-r requirements.txt (line 12)) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.1->-r requirements.txt (line 12)) (1.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 2)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 2)) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1->-r requirements.txt (line 12)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1->-r requirements.txt (line 12)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1->-r requirements.txt (line 12)) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain-groq->-r requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (1.1.0)\n",
            "Using cached transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed transformers-4.50.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from sentence_transformers import CrossEncoder\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from typing import List\n",
        "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n"
      ],
      "metadata": {
        "id": "pRHvZQIF0XsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "7f56c365-6ba4-4996-b57f-74f3f0a11ba3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-1901120588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_text_splitters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GROQ_API_KEY'] = \"---------------------------------------\""
      ],
      "metadata": {
        "id": "7YkpIWfm1Zdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature=0\n",
        "    )"
      ],
      "metadata": {
        "id": "y8QWovmJ8ED_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from a folder\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents"
      ],
      "metadata": {
        "id": "tKNEZnH_1ZbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents from a specified folder\n",
        "folder_path = \"/content/drive/MyDrive/Hybrid_RAG/books\"\n",
        "documents = load_documents(folder_path)\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")"
      ],
      "metadata": {
        "id": "pEev8K4v1vGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "id": "aiWEnJl-2I3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Create a FAISS vector store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "print(f\"Number of documents loaded and chunked: {len(splits)}\")"
      ],
      "metadata": {
        "id": "nXNRKBMFw7SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BM25 Retriever (Term-based)\n",
        "bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "bm25_retriever.k = 10 # Number of documents to retrieve for BM25\n",
        "\n",
        "#Vector Store Retriever (Vector-based)\n",
        "vectorstore_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10}) # Number of documents to retrieve for Vector Search\n",
        "\n",
        "# Create an EnsembleRetriever to combine results\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, vectorstore_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "print(\"Hybrid retriever initialized.\")"
      ],
      "metadata": {
        "id": "vKL2Sc4ux1Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_encoder_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "\n",
        "cross_encoder = HuggingFaceCrossEncoder(\n",
        "    model_name=cross_encoder_model_name,\n",
        "    model_kwargs={'device': 'cpu'} # Use 'cuda' if you have a GPU\n",
        ")\n",
        "\n",
        "# Create the CrossEncoderReranker compressor\n",
        "reranker = CrossEncoderReranker(model=cross_encoder, top_n=3)\n",
        "\n",
        "# Wrap your hybrid retriever with ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker,\n",
        "    base_retriever=hybrid_retriever\n",
        ")\n",
        "\n",
        "print(\"Reranker and compression retriever initialized.\")\n"
      ],
      "metadata": {
        "id": "LAqdGqJbx1Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based on the following context.\n",
        "If the answer is not in the context, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    print(\"\\n--- Retrieved Documents (Reranked) ---\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(f\"Content: {doc.page_content}\") # Print first 200 chars\n",
        "        if \"relevance_score\" in doc.metadata:\n",
        "            print(f\"Relevance Score: {doc.metadata['relevance_score']:.4f}\")\n",
        "        print(f\"Metadata: {doc.metadata['title']}\")\n",
        "    print(\"\\n-------------------------------------\")\n"
      ],
      "metadata": {
        "id": "8P1bT2rOzPx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"chunking strategies\""
      ],
      "metadata": {
        "id": "1ClVYMxtzaNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = compression_retriever.invoke(query)\n",
        "pretty_print_docs(retrieved_docs)"
      ],
      "metadata": {
        "id": "D-AoLgDszWjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_chain(reranking_retriever):\n",
        "    \"\"\"\n",
        "    Creates the final, history-aware RAG chain.\n",
        "    \"\"\"\n",
        "    # 3.1: Query Reformulation Chain\n",
        "    # This chain takes the user's question and chat history, then creates a\n",
        "    # new, standalone question to send to the retriever.\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood \"\n",
        "        \"without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm, reranking_retriever, contextualize_q_prompt\n",
        "    )\n",
        "\n",
        "    # 3.2: Answering Chain\n",
        "    # This chain takes the reformulated query and the retrieved documents\n",
        "    # to generate the final answer.\n",
        "    qa_system_prompt = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of retrieved context to answer the question. \"\n",
        "        \"If you don't know the answer, just say that you don't know. \"\n",
        "        \"Use three sentences maximum and keep the answer concise.\"\n",
        "        \"\\n\\n\"\n",
        "        \"{context}\"\n",
        "    )\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", qa_system_prompt),\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # This chain combines documents into a single string (\"stuffing\").\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    # 3.3: Final Retrieval Chain\n",
        "    # This orchestrates the entire process:\n",
        "    # 1. Calls the history_aware_retriever (which reformulates and retrieves).\n",
        "    # 2. Passes the retrieved documents and query to the question_answer_chain.\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    print(\"Full RAG chain created successfully.\")\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "mglHFbCY9to4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = create_final_chain(compression_retriever)\n",
        "# 4. Start the conversation\n",
        "chat_history = []\n",
        "\n",
        "print(\"\\n--- Chatbot is ready! Type 'exit' to end the conversation. ---\")\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Invoke the chain\n",
        "    result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    # Print the answer\n",
        "    print(f\"AI: {result['answer']}\")\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "    chat_history.append(AIMessage(content=result['answer']))"
      ],
      "metadata": {
        "id": "I4eOweyU3W2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tunnel Password:\")\n",
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOHWTp_mrmVn",
        "outputId": "8931ec5b-0a7e-4645-9973-f51bee574d37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunnel Password:\n",
            "34.16.153.187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Gp2p4Xr13J",
        "outputId": "8a8856be-88c7-4229-a69d-a1c6144f3a68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "added 22 packages in 4s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izw8qPn5r4gP",
        "outputId": "81dbfdec-cda8-4e24-df82-fab9ed721174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.153.187:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://sour-parts-repair.loca.lt\n",
            "/content/drive/MyDrive/Hybrid_RAG/Hybrid_RAG/app.py:89: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "2025-06-28 18:03:55.008695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751133835.032139    4638 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751133835.039209    4638 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-28 18:03:55.062678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/drive/MyDrive/Hybrid_RAG/Hybrid_RAG/node_modules/localtunnel/bin/lt.js:81\n",
            "    throw err;\n",
            "    ^\n",
            "\n",
            "Error: connection refused: localtunnel.me:27213 (check your firewall settings)\n",
            "    at Socket.<anonymous> \u001b[90m(/content/drive/MyDrive/Hybrid_RAG/Hybrid_RAG/\u001b[39mnode_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at Socket.emit (node:events:524:28)\u001b[39m\n",
            "\u001b[90m    at emitErrorNT (node:internal/streams/destroy:169:8)\u001b[39m\n",
            "\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:128:3)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\u001b[39m\n",
            "\n",
            "Node.js v20.19.0\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    }
  ]
}